---
description: DSA problem-solving workflow and automation rules
alwaysApply: true
---

# DSA Problem-Solving Workflow Rules

## ‚ö†Ô∏è CRITICAL RULE #0: NEVER MODIFY USER'S SOLUTION CODE

**BEFORE ANY ACTION, CHECK THIS RULE FIRST:**

**ABSOLUTELY FORBIDDEN:**
- NEVER modify, edit, refactor, or change the user's solution method/function
- NEVER touch the logic/algorithm code written by the user
- NEVER change formatting, spacing, or style in solution methods
- NEVER "improve" or "optimize" the user's solution code
- NEVER fix bugs in solution code unless explicitly asked

**ONLY modify solution code when:**
- User explicitly says: "fix my solution", "change my code", "update my implementation"
- User asks for debugging help on their solution
- User requests refactoring or optimization

**What you CAN modify without asking:**
- Main function and test cases (setup code)
- Documentation and comments
- Progress tracker files
- README and non-solution files

**When in doubt:** Ask the user before touching any solution method.

---

## Rule 1: Start - Problem Setup from Links

When a user shares a link to a DSA problem (LeetCode, HackerRank, etc.):

**DO:**
- Fetch and read the problem description from the link
- Create an empty solution method stub (signature only, with `// TODO` inside)
- Create reusable private helper methods for test execution to avoid repetitive code:
  - A `runTest` helper that accepts test number, input, expected output, actual output, and description
  - A `printSummary` helper that prints the final test results table and statistics
- Create a `main` function that uses the helpers to run comprehensive test cases covering:
  - All examples from the problem
  - Edge cases (empty input, single element, minimum/maximum constraints)
  - Normal cases and boundary cases
- Add detailed JavaDoc documentation including:
  - Problem number, title, and difficulty
  - Problem description and constraints
  - Placeholder for approach, time and space complexity
- Add the new problem as üîÑ In Progress in PROGRESS.md (table + daily log + category)

**Code Quality:**
- Keep test code DRY - extract common patterns into helper methods
- Each test case should be a single method call with clear parameters
- Test summary should use stored results, not inline comparisons

**DO NOT:**
- Never create the solution class or solution method logic
- Never implement the actual algorithm
- The user will implement the solution themselves

---

## Rule 2: Analyse - Evaluate User's Solution

When the user asks to analyse/evaluate their solution:

**FOCUS ONLY ON:**
- The user's solution method/algorithm implementation
- The approach and logic used to solve the problem
- Time and space complexity of the solution
- Algorithmic improvements and optimizations

**DO:**
- Read the user's solution method carefully (ignore AI-generated parts)
- List what was done RIGHT in the algorithm (correct patterns, good practices)
- List what was done WRONG or could be improved in the approach
- Analyse time and space complexity
- Suggest better algorithmic approaches with code examples and complexity comparison
- Include a comparison table of all solution approaches

**EXPLICITLY IGNORE:**
- Test cases and test infrastructure (AI-generated)
- Documentation and JavaDoc comments (AI-generated)
- Helper methods like `runTest`, `printSummary` (AI-generated)
- Main function and test setup code (AI-generated)
- Debug print statements in solution (mention to remove, but don't analyze deeply)

**DO NOT:**
- NEVER modify the user's solution code (Rule #0)
- Do not create files or update documentation at this step
- Do not spend time analyzing AI-generated test coverage or infrastructure

---

## Rule 3: Compare - Evaluate External Solutions

When the user shares another solution (from LeetCode, interviews, etc.) for comparison:

**DO:**
- Trace through the external solution step-by-step using a test case if requested
- Compare with user's solution on: correctness, complexity, readability, performance
- Explain WHY one performs better than the other
- Cover real-world factors: cache locality, constant factors, method overhead, JIT optimization
- Highlight when LeetCode benchmarks can be misleading vs production reality

**DO NOT:**
- NEVER modify the user's solution code (Rule #0)
- Do not create files or update documentation at this step

---

## Rule 4: Summarise - Consolidate Learnings

When the user asks for a summary of the current problem:

**DO:**
- Compile all learnings from the conversation into a structured summary
- Cover: problem understanding, approaches explored, evolution of solution, key insights
- Include performance analysis, pattern recognition, and best practices learned
- Highlight what to remember for future problems

**DO NOT:**
- NEVER modify the user's solution code (Rule #0)
- Do not create or update any files at this step - this is a conversation-only step

---

## Rule 5: Current - Session Resume & Status Overview

When the user wants a status check or is resuming after a break:

**DO:**
1. Read PROGRESS.md to determine:
   - Last update date
   - Overall statistics (total solved, categories, streak)
   - Any problem marked üîÑ In Progress in the Overview Table
2. If a problem is In Progress:
   - Read the problem file to check solution method status (empty TODO vs implemented)
   - Determine which workflow step was last completed:
     - **Start**: File exists, solution method is empty/TODO
     - **Solving**: Solution method has code but tests may be failing
     - **Analyse/Compare**: Solution works, user was exploring alternatives
     - **Update pending**: Solution done but docs/tracker not yet updated
     - **GitPush pending**: Everything updated but not committed
3. Present a concise status report:
   - üìä Overall Stats (total solved, streak, categories)
   - üìã Last 3 problems from Overview Table
   - üîÑ Current Problem (if any): name, link, status, last workflow step
   - üìÅ Uncommitted changes (run `git status`)
   - üéØ Suggested next action

**Format the report clearly with sections, not walls of text.**

**DO NOT:**
- Do not modify any files
- Do not start new problems
- This is a read-only status check

---

## Rule 6: Update - Documentation & Progress Tracking


When the user asks to update documentation and progress:

**DO:**
- Update JavaDoc in the problem file with:
  - Final solution approach
  - Time and space complexity
  - Key learnings and alternative approaches
  - Do NOT touch the solution code itself (Rule #0)
  - **If problem is complex:** Reference to detailed learning file (e.g., `Detailed Learning Guide: See arraysAndHashing/learnings/ProblemName-Learning.md`)
- Update PROGRESS.md:
  - Mark problem as ‚úÖ Done in the Overview Table (fill in complexity, AI help)
  - Complete the Daily Progress Log entry with approach, complexity, learnings, AI status
  - Update category checklist
  - Add new entries to Key Learnings and Patterns Identified sections
  - Update Summary Statistics (total solved, in progress, streak)
- Update README.md:
  - Update category table count
  - Update statistics section
  - Update project structure if new folders added

**Optional: Create Detailed Learning File**

When user requests additional text like "create detailed learning" or "generate learning file" OR when problem demonstrates significant complexity:

**DO:**
1. Create `<category>/learnings/` folder if it doesn't exist
2. Create `<ProblemName>-Learning.md` with comprehensive analysis:
   - **Problem Evolution Journey:** Document all attempts and iterations
   - **Solution Comparison:** Compare user's solution with alternatives (theory vs practice)
   - **Performance Deep Dive:** Explain why certain O() beats others (constant factors, JVM optimizations)
   - **Key Insights:** Theory vs practice gaps, hidden costs, optimization paradoxes
   - **Major Lessons:** Big-O limitations, constant factors, JVM intrinsics, data structure tradeoffs
   - **Best Practices:** Production recommendations, interview strategies
   - **Pattern Recognition:** Reusable patterns for similar problems
   - **Mistakes to Avoid:** Common pitfalls and bugs
3. Reference this file in the problem's JavaDoc
4. Add reference in PROGRESS.md entry

**When to create detailed learning files:**
- Problem required multiple iterations/attempts (3+ solution approaches)
- Deep performance analysis with counter-intuitive results
- Comparison of 3+ alternative approaches
- Theory vs practice paradoxes (e.g., O(n) slower than O(n log n))
- Critical algorithmic insights or patterns
- User explicitly requests it

**DO NOT:**
- Create detailed learning files for straightforward problems
- Duplicate information already in PROGRESS.md
- Create learning files unless specifically requested or complexity warrants it

---

## Rule 7: GitPush - Logical Commits and Push

When the user asks to commit and push:

**DO:**
1. Run `git status` and `git diff --stat` to analyze all changes
2. Group changes into logical, atomic commits based on their purpose:

**Commit Group 1: Problem Implementation**
- Stage problem-specific files:
  - Solution file(s) (e.g., `arraysAndHashing/TwoSum.java`)
  - Learning files (e.g., `arraysAndHashing/learnings/ProblemName-Learning.md`)
  - PROGRESS.md (problem-specific sections)
  - README.md (only statistics and category progress sections)
- Create commit message format:
  ```
  Add [Problem Name] (#X) - [Difficulty]
  
  - Implemented [approach] solution with O(time)/O(space) complexity
  - [Key learning or notable aspect]
  - Updated progress tracking and documentation
  
  Files: [list main files]
  ```

**Commit Group 2: Workflow/Infrastructure Changes**
- Stage workflow files:
  - `.cursor/rules/dsa-workflow.mdc`
  - `.cursor/commands/*.md`
  - Other infrastructure files
- Create commit message format:
  ```
  Update workflow: [brief description of changes]
  
  - [Change 1]
  - [Change 2]
  
  Files: [list files]
  ```

**Commit Group 3: Documentation Updates (if separate from problem)**
- Stage standalone documentation changes:
  - README.md (structural changes, workflow updates)
  - Other markdown files not related to specific problem
- Create commit message format:
  ```
  Docs: [brief description]
  
  - [Change 1]
  - [Change 2]
  ```

**Commit Group 4: Refactoring/Cleanup (if applicable)**
- Stage refactoring changes that don't fit above categories
- Use descriptive commit message

3. For each commit group with changes:
   - Stage files with `git add <specific files>`
   - Commit with descriptive message
   - Verify with `git status`

4. Push all commits at once: `git push origin main`
5. Final verification with `git status` to confirm clean state

**Commit Grouping Strategy:**
- **Atomic principle:** Each commit should represent one logical change
- **Reviewability:** Changes in a commit should be related and easy to review together
- **Revertability:** If needed, a commit can be reverted without affecting unrelated changes
- **Clarity:** Commit message should clearly explain the "why" not just the "what"

**Example Session:**
```
# Problem solved: Group Anagrams
Commit 1: "Add Group Anagrams (#49) - Medium
           - Implemented sorted key approach with O(nk log k) complexity
           - Created detailed learning guide covering 5 alternative approaches
           - Updated progress: 4 problems solved, 1st Medium problem"
           Files: GroupAnagrams.java, GroupAnagrams-Learning.md, PROGRESS.md, README.md

Commit 2: "Update workflow: Add detailed learning file generation
           - Extended Rule 6 (Update) with optional learning file creation
           - Updated update command with learning file documentation
           - Added criteria for when to create detailed guides"
           Files: dsa-workflow.mdc, update.md
```

**DO NOT:**
- Create one giant commit with unrelated changes
- Mix problem implementation with workflow changes in same commit
- Push without reviewing `git status` between commits

---

## Progress Tracker Format

### Problems Overview Table Format
Each row should include:
- Sequential number
- Date (YYYY-MM-DD format)
- Problem name with link and number
- Difficulty (Easy/Medium/Hard)
- Category
- Time complexity (Big O notation)
- Space complexity (Big O notation)
- AI Help (‚úÖ Yes / ‚ùå No) - **Only mark Yes if AI helped with solution/algorithm logic**
- Status (‚úÖ Done / üîÑ In Progress)

### Daily Progress Log Format
Each entry should include:
- Date and time
- Problem name and number
- Problem link
- Brief approach (1-2 sentences)
- Time complexity
- Space complexity
- AI assistance: [Yes/No] with specific details

### AI Assistance Definition
**AI assistance = ‚úÖ Yes** ONLY when:
- AI helped write the solution class/method
- AI provided the algorithm logic or approach
- AI debugged or fixed the solution code

**AI assistance = ‚ùå No** when:
- AI only created main function and test cases (this is expected)
- AI only added documentation/comments
- User implemented the entire solution independently

### Summary Statistics to Update
- Total Problems Solved count
- Categories Covered list
- Current Streak
